{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46177304-7b49-4a04-9a19-995df4172f5e",
   "metadata": {},
   "source": [
    "# The Guardian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3facd67d-34cb-4019-9e1b-8e824aa4ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb8a98-16f6-4efd-ad66-0ad3c714b596",
   "metadata": {},
   "source": [
    "## Scrape the Data from API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b19fbde-ccfc-42de-bd7a-e17a65ec981e",
   "metadata": {},
   "source": [
    "I want to scrape all the articles under the \"World\" section from November 1, 2023 and I define the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93eeb9b0-c7c3-4698-bd20-ea6bd7acce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://content.guardianapis.com/search?section=world&from-date=2023-11-01&show-blocks=all&api-key=4c043d21-d53e-4a99-a6f3-1a08745b7575&page=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "434c8ebd-85d4-48e0-96ac-23d04e996cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllist=[]\n",
    "for i in range(1,5): #115\n",
    "    a=url\n",
    "    b=str(i)\n",
    "    c=a+b\n",
    "    urllist.append(c)\n",
    "info=[]\n",
    "def json(url1):\n",
    "    response=requests.get(url1)\n",
    "    x=response.json()\n",
    "    info.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34d45622-2c18-4af2-86cc-581543cd6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[json(url1) for url1 in urllist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeee5272-1588-44d1-808b-08d8910f3292",
   "metadata": {},
   "source": [
    "I only need specific data, which includes the title, date, URL, and contents. I retrieve this data using the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "693dbc2c-75c0-4a76-ae3e-2f2e24fc6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = [\n",
    "    {\n",
    "        'webTitle': item['webTitle'],\n",
    "        'sectionName': item['sectionName'],\n",
    "        'webPublicationDate': item['webPublicationDate'],\n",
    "        'webUrl': item['webUrl'],\n",
    "        #'elements':[result['bodyTextSummary'] for result in item['blocks']['body']],\n",
    "        'elements':[{'id':  result['id'], \n",
    "                     'bodyTextSummary': result['bodyTextSummary'],                \n",
    "                     'lastModifiedDate': result['lastModifiedDate'],} for result in item['blocks']['body']],\n",
    "        #'blocks': item['blocks']['body'],\n",
    "        \n",
    "    }\n",
    "    for response in info if 'results' in response['response']\n",
    "    for item in response['response']['results']\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f451e4-229a-4406-baca-5f310a0d9f78",
   "metadata": {},
   "source": [
    "## Save corpus in text files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e116b-9b68-492a-bde6-53fcad1022cf",
   "metadata": {},
   "source": [
    "I want to use the title as the filename, so I need to remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aec32f42-494d-4ab8-b7ee-1a59f11d33de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save corpus to txt \n",
    "import os\n",
    "def cleanFilename(filename):\n",
    "    invalid_chars = '<>:\"/\\\\|?*'\n",
    "    for char in invalid_chars:\n",
    "        filename = filename.replace(char, '_')\n",
    "    return filename\n",
    "output_directory = 'guardian_corpus_spacy/'\n",
    "for line in extracted_data:\n",
    "    filename= cleanFilename(line['webTitle'])+'.txt'\n",
    "    filepath = os.path.join(output_directory, filename)\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "    # Write the data dictionary to the file\n",
    "        file.write(str(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1616fe-00b6-4687-a036-22d3060f88b3",
   "metadata": {},
   "source": [
    "## Bring corpus into csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5690ede3-d0ba-44ac-af89-affdd5946076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Filename  \\\n",
      "0  Israel-Gaza war live_ any attempt to isolate G...   \n",
      "1  Macron confident Orbán can be persuaded to sup...   \n",
      "\n",
      "                                            Document  \n",
      "0  {'webTitle': 'Israel-Gaza war live: any attemp...  \n",
      "1  {'webTitle': 'Macron confident Orbán can be pe...  \n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "# Use glob to get a list of file paths\n",
    "file_paths = glob.glob('guardian_corpus_spacy/*.txt')\n",
    "\n",
    "# Create an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each file and read its content into the DataFrame\n",
    "for file_path in file_paths:\n",
    "    # Read the content of the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text_content = file.read()\n",
    "        #print(text_content[:20])\n",
    "    # Extract the filename from the file path\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    # Create a DataFrame with the filename and text content\n",
    "    df = pd.DataFrame({'Filename': [filename], 'Document': [text_content]})\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    dfs.append(df)\n",
    "    \n",
    "# Concatenate all DataFrames into one\n",
    "corpus_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "corpus_df.to_csv('Guardian_spacy.csv', index=False)\n",
    "print(corpus_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97ac1a-3984-45b5-9f95-8f61c2574f9d",
   "metadata": {},
   "source": [
    "## Get Tokens with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2046e4d-c033-4328-9f17-55a9eedc4756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    {'webTitle': 'Israel-Gaza war live: any attemp...\n",
      "1    {'webTitle': 'Macron confident Orbán can be pe...\n",
      "Name: Tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Guardian_spacy.csv')\n",
    "df['Token'] = df['Document'].copy()\n",
    "print(df['Token'].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60f883c6-0a53-4873-9319-08c911510e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    (webTitle, IsraelGaza, war, live, any, attempt...\n",
      "1    (webTitle, Macron, confident, Orbán, can, be, ...\n",
      "Name: Token, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def get_token(text): \n",
    "    punctuation = '!@#$%^&*()_-+={}[]:;\"\\'|<>,.?/~`'\n",
    "    text = ''.join(character for character in text\n",
    "                   if character not in punctuation)\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    return doc\n",
    "\n",
    "df['Token'] = df['Document'].apply(get_token)\n",
    "\n",
    "print(df['Token'].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5754e8-e158-4cd0-831e-9d4e03e49f9c",
   "metadata": {},
   "source": [
    "## Get Lemmas with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a62d7ac-4943-48a7-8684-68baf0a6707f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [webTitle, IsraelGaza, war, live, any, attempt...\n",
      "1    [webTitle, Macron, confident, Orbán, can, be, ...\n",
      "Name: Lemma, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def lemma(text):\n",
    "    return [(token.lemma_) for token in text]\n",
    "df['Lemma'] = df['Token'].apply(lemma)\n",
    "print(df['Lemma'].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f818954-c0a5-47cb-a343-1599341a4f06",
   "metadata": {},
   "source": [
    "## Get POS with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21ffe0b5-a6d3-4302-8beb-c967345417bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [PROPN, PROPN, NOUN, VERB, DET, NOUN, PART, VE...\n",
      "1    [PROPN, PROPN, ADJ, PROPN, AUX, AUX, VERB, PAR...\n",
      "Name: Pos, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def pos(text):\n",
    "    return [(token.pos_) for token in text]\n",
    "\n",
    "df['Pos'] = df['Token'].apply(pos)\n",
    "print(df['Pos'].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1e5681b-12ab-41f6-af0c-b3b7dafa53f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Filename  \\\n",
      "0  Israel-Gaza war live_ any attempt to isolate G...   \n",
      "1  Macron confident Orbán can be persuaded to sup...   \n",
      "\n",
      "                                            Document  \\\n",
      "0  {'webTitle': 'Israel-Gaza war live: any attemp...   \n",
      "1  {'webTitle': 'Macron confident Orbán can be pe...   \n",
      "\n",
      "                                               Token  \\\n",
      "0  (webTitle, IsraelGaza, war, live, any, attempt...   \n",
      "1  (webTitle, Macron, confident, Orbán, can, be, ...   \n",
      "\n",
      "                                                 Pos  \\\n",
      "0  [PROPN, PROPN, NOUN, VERB, DET, NOUN, PART, VE...   \n",
      "1  [PROPN, PROPN, ADJ, PROPN, AUX, AUX, VERB, PAR...   \n",
      "\n",
      "                                               Lemma  \n",
      "0  [webTitle, IsraelGaza, war, live, any, attempt...  \n",
      "1  [webTitle, Macron, confident, Orbán, can, be, ...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ed5dd7e-9a68-470d-8dc9-763e7cdacf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Guardian_pandas_spacy_03.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
